# ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ç¾åœ¨ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã›ã‚“
# ç¾åœ¨ã®å®Ÿè£…ã§ã¯OpenAI Realtime APIã‚’ç›´æ¥ä½¿ç”¨ã—ã¦ãŠã‚Šã€
# Django Channelsã®WebSocketã¯ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã›ã‚“

# å°†æ¥çš„ã«WebSocketæ©Ÿèƒ½ãŒå¿…è¦ã«ãªã£ãŸå ´åˆã«å‚™ãˆã¦ã€
# ã‚³ãƒ¼ãƒ‰ã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ä¿æŒã—ã¾ã™

"""
import json
import base64
import logging
import io
from typing import Dict, Any
from channels.generic.websocket import AsyncWebsocketConsumer
from channels.db import database_sync_to_async
from .services import AudioProcessor
import asyncio

logger = logging.getLogger(__name__)


class InterviewConsumer(AsyncWebsocketConsumer):
    async def connect(self):
        self.session_id = self.scope['url_route']['kwargs']['session_id']
        self.session_group_name = f'interview_{self.session_id}'
        
        await self.channel_layer.group_add(
            self.session_group_name,
            self.channel_name
        )
        # æ—¢å­˜ã®ASGIã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’å–å¾—ï¼ˆæ–°è¦ä½œæˆã—ãªã„ï¼‰
        loop = asyncio.get_running_loop()
        # æ¥ç¶šå˜ä½ã®AudioProcessorï¼ˆå†…éƒ¨ã§Realtimeã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ç¢ºç«‹ï¼‰
        self.audio_processor = AudioProcessor(loop=loop, language="ja")
        logger.info("âœ… InterviewConsumer connected & AudioProcessor ready")
        self.pending_metadata = None  # ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿å¾…ã¡ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        await self.accept()
        logger.info(f"WebSocket connected for session {self.session_id}")
    
    async def disconnect(self, close_code: int) -> None:
        try:
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å¾Œå§‹æœ«
            await self.channel_layer.group_discard(
                self.session_group_name,
                self.channel_name
            )
            await database_sync_to_async(self.audio_processor.close)()
        except Exception as e:
            logger.warning(f"close error: {e}")
        logger.info(f"WebSocket disconnected for session {self.session_id}")

    async def receive(self, text_data: str = None, bytes_data: bytes = None) -> None:
        try:
            if text_data:
                # JSONãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†
                data = json.loads(text_data)
                message_type = data.get('type')
                
                if message_type == 'audio_metadata':
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¦ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã‚’å¾…ã¤
                    self.pending_metadata = data
                    logger.info(f"ğŸ“‹ Audio metadata received: chunk {data.get('chunk_id')}, size: {data.get('size')}B")
                elif message_type == 'audio_chunk':
                    # å¾“æ¥ã®Base64å½¢å¼
                    await self.handle_audio_chunk(data)
                else:
                    logger.warning(f"Unknown message type: {message_type}")
                    
            elif bytes_data:
                # ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿å‡¦ç†
                if self.pending_metadata:
                    await self.handle_binary_audio_chunk(self.pending_metadata, bytes_data)
                    self.pending_metadata = None
                else:
                    logger.warning("Received binary data without metadata")
                
        except Exception as e:
            logger.error(f"Error processing message: {e}")
            await self.send(text_data=json.dumps({
                'type': 'error',
                'message': str(e)
            }))

    
    
    async def handle_binary_audio_chunk(self, metadata: Dict[str, Any], audio_bytes: bytes) -> None:
        # ãƒã‚¤ãƒŠãƒªéŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†ï¼ˆWAVãƒ•ã‚¡ã‚¤ãƒ«ç›´æ¥ï¼‰
        chunk_id = metadata.get('chunk_id', 0)
        audio_format = metadata.get('format', 'unknown')
        expected_size = metadata.get('size', 0)
        
        logger.info(f"ğŸ¤ Binary chunk {chunk_id}: format={audio_format}, expected={expected_size}B, actual={len(audio_bytes)}B")
        
        try:
            if audio_bytes and len(audio_bytes) > 0:
                if audio_format == 'wav':
                    # ãƒã‚¤ãƒŠãƒªWAVãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒ ã§å‡¦ç†
                    audio_stream = io.BytesIO(audio_bytes)
                    transcribed_text = await database_sync_to_async(
                        self.audio_processor.transcribe_wav_binary_stream
                    )(audio_stream)
                else:
                    # ä»–ã®å½¢å¼ã®å ´åˆã¯Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦å¾“æ¥å‡¦ç†
                    audio_data = base64.b64encode(audio_bytes).decode('utf-8')
                    transcribed_text = await database_sync_to_async(
                        self.audio_processor.transcribe_audio
                    )(audio_data)
                
                if transcribed_text and transcribed_text.strip():
                    await self.send(text_data=json.dumps({
                        'type': 'transcription_result',
                        'text': transcribed_text.strip(),
                        'is_final': False,
                        'chunk_id': chunk_id
                    }))
                    logger.info(f"âœ… Binary transcription: {transcribed_text[:50]}...")
                else:
                    logger.warning(f"âš ï¸ No transcription result for binary chunk {chunk_id}")
            else:
                logger.warning(f"âš ï¸ Empty binary data for chunk {chunk_id}")
                    
        except Exception as e:
            logger.error(f"âŒ Binary audio processing error for chunk {chunk_id}: {e}")
            await self.send(text_data=json.dumps({
                'type': 'audio_error',
                'message': f'Binary audio processing failed: {str(e)}',
                'chunk_id': chunk_id
            }))

    async def handle_audio_chunk(self, data: Dict[str, Any]) -> None:
        # éŸ³å£°ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†ï¼ˆWAVãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼‰
        audio_data = data.get('audio_data')
        is_final = data.get('is_final', False)
        chunk_id = data.get('chunk_id', 0)
        audio_format = data.get('format', 'unknown')
        
        logger.info(f"ğŸ¤ Chunk {chunk_id}: final={is_final}, format={audio_format}, size={len(base64.b64decode(audio_data)) if audio_data else 0}B")
        
        try:
            if audio_data:
                # WAVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
                if audio_format == 'wav':
                    # WAVãƒ•ã‚¡ã‚¤ãƒ«ã¯ç›´æ¥OpenAI APIã«é€ä¿¡å¯èƒ½
                    transcribed_text = await database_sync_to_async(
                        self.audio_processor.transcribe_wav_direct
                    )(audio_data)
                else:
                    # å¾“æ¥ã®å‡¦ç†ï¼ˆPydubçµŒç”±ï¼‰
                    transcribed_text = await database_sync_to_async(
                        self.audio_processor.transcribe_audio
                    )(audio_data)
                
                if transcribed_text and transcribed_text.strip():
                    await self.send(text_data=json.dumps({
                        'type': 'transcription_result',
                        'text': transcribed_text.strip(),
                        'is_final': is_final,
                        'chunk_id': chunk_id
                    }))
                    logger.info(f"âœ… Transcription: {transcribed_text[:50]}... (final: {is_final})")
                else:
                    logger.warning(f"âš ï¸ No transcription result for chunk {chunk_id}")
            else:
                # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯å—ä¿¡ç¢ºèªã®ã¿
                await self.send(text_data=json.dumps({
                    'type': 'audio_received',
                    'status': 'received',
                    'chunk_id': chunk_id
                }))
                    
        except Exception as e:
            logger.error(f"âŒ Audio processing error for chunk {chunk_id}: {e}")
            await self.send(text_data=json.dumps({
                'type': 'audio_error',
                'message': f'Audio processing failed: {str(e)}',
                'chunk_id': chunk_id
            }))
"""
